# Neural Network Optimization & Activation Function Analysis (Python)

A Python project exploring neural network training dynamics by implementing feedforward networks from scratch and analyzing how activation functions and network depth affect cost minimization and convergence.

## Experiments Conducted

- Implemented forward and backpropagation using gradient descent
- Compared sigmoid and ReLU activation functions
- Tested shallow vs deeper network architectures
- Visualized cost reduction over training iterations

## Key Findings

- ReLU converges faster than sigmoid and avoids vanishing gradient issues
- Sigmoid networks require more tuning and iterations
- Additional hidden layers improve learning capacity but increase optimization complexity
- Convergence behavior depends strongly on activation choice and architecture

## Tools & Skills Used

- Python
- NumPy
- Matplotlib
- Gradient descent optimization
- Manual backpropagation
- Model diagnostics

## Notes

This project demonstrates foundational understanding of neural network mechanics and optimization behavior, supporting broader data science and analytics work.

## Author

Mohammed Zareef-Mustafa
